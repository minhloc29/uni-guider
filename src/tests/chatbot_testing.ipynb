{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Documents/Code/uni-guider/src\n",
      "/Users/macbook/Documents/Code/uni-guider\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/macbook/Documents/Code/uni-guider'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%cd ..\n",
    "%cd ..\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from src.chatbot.utils import *\n",
    "import numpy as np\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from uuid import uuid4\n",
    "# from rag.prompt import template\n",
    "from src.chatbot.rag.config import LLM_CONFIG, EMBEDDING_CONFIG\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from src.chatbot.rag.retrieval import retrieve_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc_from_json(filename=\"data/chunking_documents.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:29:12.067156Z",
     "start_time": "2024-11-30T07:28:39.100454Z"
    }
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_CONFIG[\"model_name\"],\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "#AIzaSyCbFc_ba1TN1fW_DeCIiR_zzszZdCQBrU0\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAZSjeZMOA6igO9MiVLBXoFEdjBiyNYiSA\"\n",
    "# Create the model\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(hf.embed_query(\"Xin chào.\")))\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=hf,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(data))]\n",
    "vectorstore.add_documents(documents=data, ids=uuids)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Bạn là một cô gái hướng dẫn cực kì dễ thương tên là Mei Mei, một bé đáng yêu siêu bánh bèo, chuyên giải đáp thắc mắc cho các bạn sinh viên Bách Khoa về trường của họ. Trả lời dựa vào thông tin được cung cấp dưới đây và tuyệt đối không trả lời ngoài ngữ cảnh được cung cấp nhé.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Bạn muốn hỏi gì: \")\n",
    "    if question.lower() in ['/exit']:\n",
    "        print(\"Tạm biệt nhé tình iu của Mei, hẹn gặp bạn lần sau!\")\n",
    "        break\n",
    "    \n",
    "    filtered_docs = retrieve_documents(question)\n",
    "    reference_urls = []\n",
    "    for doc in filtered_docs:\n",
    "        reference_urls.append(doc.metadata['source'])\n",
    "        \n",
    "    doc_txts = \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "    # prompt = rag_prompt.format(context = doc_txts)\n",
    "    print(f\"Length of doc is: {len(doc_txts.split())}\")\n",
    "    query = [SystemMessage(f\"{template}\\nNgữ cảnh: \\n{doc_txts}\"), HumanMessage(question)]\n",
    "    response = llm.invoke(query)\n",
    "    print(response.content)\n",
    "    print(\"Reference URLs:\")\n",
    "    for url in reference_urls:\n",
    "        print(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611111111111111\n",
      "Hit Rate@2: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "def hit_rate_at_k(predictions, ground_truth, k):\n",
    "    hits = 0\n",
    "    for preds, truth in zip(predictions, ground_truth):\n",
    "        if truth in preds[:k]:\n",
    "            hits += 1\n",
    "    return hits / len(ground_truth)\n",
    "\n",
    "responses = [\n",
    "    [\"O(1) time complexity is optimal.\", \"Other option A\", \"Other option B\"],\n",
    "    [\"Other option A\", \"Other option B\", \"Merge Sort is a divide-and-conquer algorithm.\"],\n",
    "    [\"Other option A\", \"Dynamic Programming is key for optimization problems.\", \"Other option B\"]\n",
    "]\n",
    "\n",
    "ground_truth = [\n",
    "    \"O(1) time complexity is optimal.\",\n",
    "    \"Merge Sort is a divide-and-conquer algorithm.\",\n",
    "    \"Dynamic Programming is key for optimization problems.\"\n",
    "]\n",
    "\n",
    "print(calculate_mrr(responses, ground_truth))  # Ou\n",
    "# Example: Hit Rate@2\n",
    "print(\"Hit Rate@2:\", hit_rate_at_k(responses, ground_truth, k=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_guider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
