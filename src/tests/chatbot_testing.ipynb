{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/macbook/Documents/Code/uni-guider/src\n",
      "/Users/macbook/Documents/Code/uni-guider\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/macbook/Documents/Code/uni-guider'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%cd ..\n",
    "%cd ..\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from src.chatbot.utils import *\n",
    "import numpy as np\n",
    "from pyvi.ViTokenizer import tokenize\n",
    "from uuid import uuid4\n",
    "# from rag.prompt import template\n",
    "from src.chatbot.rag.config import LLM_CONFIG, EMBEDDING_CONFIG\n",
    "from pydantic import BaseModel\n",
    "from sentence_transformers import CrossEncoder\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_doc_from_json(filename=\"data/document_langchain.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:29:12.067156Z",
     "start_time": "2024-11-30T07:28:39.100454Z"
    }
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_CONFIG[\"model_name\"],\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model = LLM_CONFIG[\"model_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(len(hf.embed_query(\"Xin chào.\")))\n",
    "vectorstore = FAISS(\n",
    "    embedding_function=hf,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "uuids = [str(uuid4()) for _ in range(len(data))]\n",
    "vectorstore.add_documents(documents=data, ids=uuids)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Bảo hiểm y tế của Bách Khoa\"\n",
    "MODEL_ID = 'itdainb/PhoRanker'\n",
    "MAX_LENGTH = 512\n",
    "model = CrossEncoder(MODEL_ID, max_length=MAX_LENGTH)\n",
    "model.model.half()\n",
    "\n",
    "def retrieve_documents(query: str) -> List[Document]:\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    segmented_question = tokenize(query)\n",
    "    segmented_documents = [tokenize(doc.page_content) for doc in docs]\n",
    "    tokenized_pairs = [[segmented_question, sent] for sent in segmented_documents]\n",
    "\n",
    "    scores = model.predict(tokenized_pairs)\n",
    "    top_ids = np.argsort(scores)[::-1][:5]\n",
    "    top_documents = [docs[i] for i in top_ids]\n",
    "    return top_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Bạn là một cô gái hướng dẫn dễ thương tên là Mei Mei, đáng yêu đang trả lời thắc mắc của các bạn sinh viên Bách Khoa về trường của họ.\n",
    "Bạn hãy trả lời lại thông tin sau đây theo giọng điệu đáng yêu nhí nhảnh của mình nhé.\n",
    "Bạn chỉ được trả lời dựa theo ngữ cảnh đã được cung cấp và tuyệt đối không được trả lời vượt qua phạm vi ngữ cảnh.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    question = input(\"Bạn muốn hỏi gì: \")\n",
    "    if question.lower() in ['/exit']:\n",
    "        print(\"Tạm biệt nhé tình iu của Mei, hẹn gặp bạn lần sau!\")\n",
    "        break\n",
    "    \n",
    "    filtered_docs = retrieve_documents(question)\n",
    "    doc_txts = \"\\n\\n\".join(doc.page_content for doc in filtered_docs)\n",
    "    # prompt = rag_prompt.format(context = doc_txts)\n",
    "    print(f\"Length of doc is: {len(doc_txts.split())}\")\n",
    "    query = [SystemMessage(f\"{template}\\nNgữ cảnh: \\n{doc_txts}\"), HumanMessage(question)]\n",
    "    response = llm.invoke(query)\n",
    "    print(response.content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate_response(request: QueryRequest):\n",
    "    query = request.query\n",
    "    llm = ChatOllama(model = \"llama3.2:3b-instruct-fp16\")\n",
    "    response = llm.invoke(query)\n",
    "    text_response = response.content\n",
    "    return {\"response\": text_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ba', 'la', 'cay', 'nen', 'vang.', 'me', 'la', 'cay', 'nen!', 'xanh.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'ba la cay nen vang. me la cay nen! xanh.'\n",
    "y = x.split()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni_guider",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
